[2025-07-27T19:52:13.170+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: github_events_elt_pipeline.extract_and_load_to_staging manual__2025-07-27T19:40:46.201979+00:00 [queued]>
[2025-07-27T19:52:13.197+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: github_events_elt_pipeline.extract_and_load_to_staging manual__2025-07-27T19:40:46.201979+00:00 [queued]>
[2025-07-27T19:52:13.199+0000] {taskinstance.py:2193} INFO - Starting attempt 3 of 3
[2025-07-27T19:52:13.271+0000] {taskinstance.py:2217} INFO - Executing <Task(KubernetesPodOperator): extract_and_load_to_staging> on 2025-07-27 19:40:46.201979+00:00
[2025-07-27T19:52:13.279+0000] {standard_task_runner.py:60} INFO - Started process 189 to run task
[2025-07-27T19:52:13.284+0000] {standard_task_runner.py:87} INFO - Running: ['***', 'tasks', 'run', 'github_events_elt_pipeline', 'extract_and_load_to_staging', 'manual__2025-07-27T19:40:46.201979+00:00', '--job-id', '8', '--raw', '--subdir', 'DAGS_FOLDER/github_events_elt_pipeline.py', '--cfg-path', '/tmp/tmp8mhsuil1']
[2025-07-27T19:52:13.287+0000] {standard_task_runner.py:88} INFO - Job 8: Subtask extract_and_load_to_staging
[2025-07-27T19:52:13.315+0000] {logging_mixin.py:188} WARNING - /home/***/.local/lib/python3.10/site-packages/***/settings.py:194 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
[2025-07-27T19:52:13.378+0000] {task_command.py:423} INFO - Running <TaskInstance: github_events_elt_pipeline.extract_and_load_to_staging manual__2025-07-27T19:40:46.201979+00:00 [running]> on host 1c730154e441
[2025-07-27T19:52:13.527+0000] {taskinstance.py:2513} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='github_events_elt_pipeline' AIRFLOW_CTX_TASK_ID='extract_and_load_to_staging' AIRFLOW_CTX_EXECUTION_DATE='2025-07-27T19:40:46.201979+00:00' AIRFLOW_CTX_TRY_NUMBER='3' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-07-27T19:40:46.201979+00:00'
[2025-07-27T19:52:13.543+0000] {pod.py:1076} INFO - Building pod kafka-consumer-pod-hu5aj4bg with labels: {'dag_id': 'github_events_elt_pipeline', 'task_id': 'extract_and_load_to_staging', 'run_id': 'manual__2025-07-27T194046.2019790000-fccb62a8d', 'kubernetes_pod_operator': 'True', 'try_number': '3'}
[2025-07-27T19:52:13.559+0000] {base.py:83} INFO - Using connection ID 'kubernetes_default' for task execution.
[2025-07-27T19:52:13.583+0000] {taskinstance.py:2731} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/cncf/kubernetes/hooks/kubernetes.py", line 265, in _get_default_client
    config.load_incluster_config(client_configuration=self.client_configuration)
  File "/home/airflow/.local/lib/python3.10/site-packages/kubernetes/config/incluster_config.py", line 121, in load_incluster_config
    try_refresh_token=try_refresh_token).load_and_set(client_configuration)
  File "/home/airflow/.local/lib/python3.10/site-packages/kubernetes/config/incluster_config.py", line 54, in load_and_set
    self._load_config()
  File "/home/airflow/.local/lib/python3.10/site-packages/kubernetes/config/incluster_config.py", line 62, in _load_config
    raise ConfigException("Service host/port is not set.")
kubernetes.config.config_exception.ConfigException: Service host/port is not set.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 444, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 414, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py", line 578, in execute
    return self.execute_sync(context)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py", line 584, in execute_sync
    self.pod_request_obj = self.build_pod_request_obj(context)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py", line 1085, in build_pod_request_obj
    "airflow_kpo_in_cluster": str(self.hook.is_in_cluster),
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/cncf/kubernetes/hooks/kubernetes.py", line 281, in is_in_cluster
    self.api_client  # so we can determine if we are in_cluster or not
  File "/usr/local/lib/python3.10/functools.py", line 981, in __get__
    val = self.func(instance)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/cncf/kubernetes/hooks/kubernetes.py", line 289, in api_client
    return self.get_conn()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/cncf/kubernetes/hooks/kubernetes.py", line 257, in get_conn
    return self._get_default_client(cluster_context=cluster_context)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/cncf/kubernetes/hooks/kubernetes.py", line 270, in _get_default_client
    config.load_kube_config(
  File "/home/airflow/.local/lib/python3.10/site-packages/kubernetes/config/kube_config.py", line 822, in load_kube_config
    loader.load_and_set(config)
  File "/home/airflow/.local/lib/python3.10/site-packages/kubernetes/config/kube_config.py", line 590, in load_and_set
    self._load_cluster_info()
  File "/home/airflow/.local/lib/python3.10/site-packages/kubernetes/config/kube_config.py", line 555, in _load_cluster_info
    temp_file_path=self._temp_file_path).as_file()
  File "/home/airflow/.local/lib/python3.10/site-packages/kubernetes/config/kube_config.py", line 129, in as_file
    raise ConfigException("File does not exist: %s" % self._file)
kubernetes.config.config_exception.ConfigException: File does not exist: /home/***/.kube/C:\Users\idaba\.minikube\ca.crt
[2025-07-27T19:52:13.600+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=github_events_elt_pipeline, task_id=extract_and_load_to_staging, execution_date=20250727T194046, start_date=20250727T195213, end_date=20250727T195213
[2025-07-27T19:52:13.660+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 8 for task extract_and_load_to_staging (File does not exist: /home/***/.kube/C:\Users\idaba\.minikube\ca.crt; 189)
[2025-07-27T19:52:13.700+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2025-07-27T19:52:13.743+0000] {taskinstance.py:3312} INFO - 0 downstream tasks scheduled from follow-on schedule check
