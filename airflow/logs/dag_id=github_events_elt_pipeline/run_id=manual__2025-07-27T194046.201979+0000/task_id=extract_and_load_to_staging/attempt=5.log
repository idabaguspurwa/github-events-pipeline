[2025-07-28T11:14:36.595+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: github_events_elt_pipeline.extract_and_load_to_staging manual__2025-07-27T19:40:46.201979+00:00 [queued]>
[2025-07-28T11:14:36.608+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: github_events_elt_pipeline.extract_and_load_to_staging manual__2025-07-27T19:40:46.201979+00:00 [queued]>
[2025-07-28T11:14:36.609+0000] {taskinstance.py:2193} INFO - Starting attempt 5 of 5
[2025-07-28T11:14:36.633+0000] {taskinstance.py:2217} INFO - Executing <Task(KubernetesPodOperator): extract_and_load_to_staging> on 2025-07-27 19:40:46.201979+00:00
[2025-07-28T11:14:36.640+0000] {standard_task_runner.py:60} INFO - Started process 285 to run task
[2025-07-28T11:14:36.646+0000] {standard_task_runner.py:87} INFO - Running: ['***', 'tasks', 'run', 'github_events_elt_pipeline', 'extract_and_load_to_staging', 'manual__2025-07-27T19:40:46.201979+00:00', '--job-id', '43', '--raw', '--subdir', 'DAGS_FOLDER/github_events_elt_pipeline.py', '--cfg-path', '/tmp/tmpaq57ae95']
[2025-07-28T11:14:36.650+0000] {standard_task_runner.py:88} INFO - Job 43: Subtask extract_and_load_to_staging
[2025-07-28T11:14:36.681+0000] {logging_mixin.py:188} WARNING - /home/***/.local/lib/python3.10/site-packages/***/settings.py:194 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
[2025-07-28T11:14:36.743+0000] {task_command.py:423} INFO - Running <TaskInstance: github_events_elt_pipeline.extract_and_load_to_staging manual__2025-07-27T19:40:46.201979+00:00 [running]> on host 1c730154e441
[2025-07-28T11:14:36.875+0000] {taskinstance.py:2513} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='github_events_elt_pipeline' AIRFLOW_CTX_TASK_ID='extract_and_load_to_staging' AIRFLOW_CTX_EXECUTION_DATE='2025-07-27T19:40:46.201979+00:00' AIRFLOW_CTX_TRY_NUMBER='5' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-07-27T19:40:46.201979+00:00'
[2025-07-28T11:14:36.889+0000] {pod.py:1076} INFO - Building pod kafka-consumer-pod-2an5klh6 with labels: {'dag_id': 'github_events_elt_pipeline', 'task_id': 'extract_and_load_to_staging', 'run_id': 'manual__2025-07-27T194046.2019790000-fccb62a8d', 'kubernetes_pod_operator': 'True', 'try_number': '5'}
[2025-07-28T11:14:36.908+0000] {base.py:83} INFO - Using connection ID 'kubernetes_default' for task execution.
[2025-07-28T11:14:36.912+0000] {taskinstance.py:2731} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 444, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 414, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py", line 578, in execute
    return self.execute_sync(context)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py", line 584, in execute_sync
    self.pod_request_obj = self.build_pod_request_obj(context)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py", line 1085, in build_pod_request_obj
    "airflow_kpo_in_cluster": str(self.hook.is_in_cluster),
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/cncf/kubernetes/hooks/kubernetes.py", line 281, in is_in_cluster
    self.api_client  # so we can determine if we are in_cluster or not
  File "/usr/local/lib/python3.10/functools.py", line 981, in __get__
    val = self.func(instance)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/cncf/kubernetes/hooks/kubernetes.py", line 289, in api_client
    return self.get_conn()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/cncf/kubernetes/hooks/kubernetes.py", line 250, in get_conn
    config.load_kube_config(
  File "/home/airflow/.local/lib/python3.10/site-packages/kubernetes/config/kube_config.py", line 815, in load_kube_config
    loader = _get_kube_config_loader(
  File "/home/airflow/.local/lib/python3.10/site-packages/kubernetes/config/kube_config.py", line 767, in _get_kube_config_loader
    kcfg = KubeConfigMerger(filename)
  File "/home/airflow/.local/lib/python3.10/site-packages/kubernetes/config/kube_config.py", line 686, in __init__
    self._load_config_from_file_path(paths)
  File "/home/airflow/.local/lib/python3.10/site-packages/kubernetes/config/kube_config.py", line 711, in _load_config_from_file_path
    self.load_config(path)
  File "/home/airflow/.local/lib/python3.10/site-packages/kubernetes/config/kube_config.py", line 716, in load_config
    config = yaml.safe_load(f)
  File "/home/airflow/.local/lib/python3.10/site-packages/yaml/__init__.py", line 125, in safe_load
    return load(stream, SafeLoader)
  File "/home/airflow/.local/lib/python3.10/site-packages/yaml/__init__.py", line 81, in load
    return loader.get_single_data()
  File "/home/airflow/.local/lib/python3.10/site-packages/yaml/constructor.py", line 49, in get_single_data
    node = self.get_single_node()
  File "/home/airflow/.local/lib/python3.10/site-packages/yaml/composer.py", line 36, in get_single_node
    document = self.compose_document()
  File "/home/airflow/.local/lib/python3.10/site-packages/yaml/composer.py", line 55, in compose_document
    node = self.compose_node(None, None)
  File "/home/airflow/.local/lib/python3.10/site-packages/yaml/composer.py", line 84, in compose_node
    node = self.compose_mapping_node(anchor)
  File "/home/airflow/.local/lib/python3.10/site-packages/yaml/composer.py", line 127, in compose_mapping_node
    while not self.check_event(MappingEndEvent):
  File "/home/airflow/.local/lib/python3.10/site-packages/yaml/parser.py", line 98, in check_event
    self.current_event = self.state()
  File "/home/airflow/.local/lib/python3.10/site-packages/yaml/parser.py", line 428, in parse_block_mapping_key
    if self.check_token(KeyToken):
  File "/home/airflow/.local/lib/python3.10/site-packages/yaml/scanner.py", line 116, in check_token
    self.fetch_more_tokens()
  File "/home/airflow/.local/lib/python3.10/site-packages/yaml/scanner.py", line 223, in fetch_more_tokens
    return self.fetch_value()
  File "/home/airflow/.local/lib/python3.10/site-packages/yaml/scanner.py", line 577, in fetch_value
    raise ScannerError(None, None,
yaml.scanner.ScannerError: mapping values are not allowed here
  in "/tmp/tmp772xvqld", line 1, column 24
[2025-07-28T11:14:36.933+0000] {taskinstance.py:1149} INFO - Marking task as FAILED. dag_id=github_events_elt_pipeline, task_id=extract_and_load_to_staging, execution_date=20250727T194046, start_date=20250728T111436, end_date=20250728T111436
[2025-07-28T11:14:36.952+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 43 for task extract_and_load_to_staging (mapping values are not allowed here
  in "/tmp/tmp772xvqld", line 1, column 24; 285)
[2025-07-28T11:14:36.980+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2025-07-28T11:14:37.018+0000] {taskinstance.py:3312} INFO - 0 downstream tasks scheduled from follow-on schedule check
